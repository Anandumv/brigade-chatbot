import unittest
from unittest.mock import MagicMock, patch
import sys
import os
import json

# Add backend to path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from services.flow_engine import flow_engine
from services.hybrid_retrieval import hybrid_retrieval

class TestProductionReadiness(unittest.TestCase):
    def setUp(self):
        flow_engine.sessions = {}
        self.session_id = "prod_user_999"
        # Ensure mock data
        if not hybrid_retrieval.mock_projects:
            hybrid_retrieval._load_mock_data()

    @patch('services.flow_engine.openai.OpenAI')
    @patch('services.flow_engine.get_projects_table')
    def test_complex_comparison_flow(self, mock_get_table, mock_openai):
        """
        Scenario: User compares two locations.
        User: "Compare Whitefield vs Sarjapur"
        Expectation: Persuasive text comparison generated by LLM.
        """
        print("\n--- TEST: COMPARISON FLOW ---")
        mock_get_table.return_value = None
        mock_client = MagicMock()
        mock_openai.return_value = mock_client

        # Mocks
        mock_client.chat.completions.create.side_effect = [
            # Extraction
            MagicMock(choices=[MagicMock(message=MagicMock(content=json.dumps({})))]),
            # Intent: Comparison
            MagicMock(choices=[MagicMock(message=MagicMock(content=json.dumps({
                "intent": "comparison",
                "confidence": 0.9
            })))]),
            # Generation (Persuasion)
            MagicMock(choices=[MagicMock(message=MagicMock(content="Whitefield offers better metro connectivity and IT parks, while Sarjapur offers more lush green spaces and larger villa options for the same price."))])
        ]

        response = flow_engine.process(self.session_id, "Compare Whitefield vs Sarjapur")
        print(f"System Action: {response.system_action}")
        
        self.assertIn("Whitefield", response.system_action)
        self.assertIn("Sarjapur", response.system_action)
        self.assertIn("connectivity", response.system_action)

    @patch('services.flow_engine.sales_conversation.handle_sales_query')
    @patch('services.flow_engine.openai.OpenAI')
    def test_objection_handling(self, mock_openai, mock_handle_sales):
        """
        Scenario: User says "Too expensive".
        Expectation: Bot routes to Objection Handler logic.
        """
        print("\n--- TEST: OBJECTION HANDLING ---")
        mock_client = MagicMock()
        mock_openai.return_value = mock_client

        # Mocks
        mock_client.chat.completions.create.side_effect = [
            # Extraction
            MagicMock(choices=[MagicMock(message=MagicMock(content=json.dumps({})))]),
            # Intent: Objection
            MagicMock(choices=[MagicMock(message=MagicMock(content=json.dumps({
                "intent": "objection_budget",
                "confidence": 0.95
            })))]),
            # Fallback Persuasion (if handle_sales_query returns None, but we mock it)
        ]
        
        # Mock the sales conversation module response
        mock_handle_sales.return_value = ("I understand price is a concern. However, considering the 12% annual appreciation in this area, it's a strategic investment.", None, None, None)

        response = flow_engine.process(self.session_id, "It is too expensive")
        print(f"System Action: {response.system_action}")
        
        self.assertIn("strategic investment", response.system_action)

    @patch('services.flow_engine.openai.OpenAI')
    @patch('services.flow_engine.get_projects_table')
    def test_error_resilience_llm_failure(self, mock_get_table, mock_openai):
        """
        Scenario: LLM returns garbage JSON or fails.
        Expectation: System does not crash, falls back to Ambiguous/Safe response.
        """
        print("\n--- TEST: ERROR RESILIENCE (LLM CRASH) ---")
        mock_get_table.return_value = None
        mock_client = MagicMock()
        mock_openai.return_value = mock_client

        # Mocks - Raise Exception on LLM call
        mock_client.chat.completions.create.side_effect = Exception("OpenAI API Down")

        # This should not raise exception but return safe response
        try:
            response = flow_engine.process(self.session_id, "Find me a home")
            print(f"System Action (Fallback): {response.system_action}")
            # Logic: If extraction fails, it returns empty requirements. 
            # If intent classification fails (fallback handled in code?), it returns 'ambiguous' usually or log error.
            # Let's check flow_engine.py error handling.
            
            # Current flow_engine `classify_user_intent` catches exception and falls back to keywords!
            # So "Find me a home" -> might match nothing or "ambiguous".
            
            self.assertTrue(len(response.system_action) > 0)
        except Exception as e:
            self.fail(f"System crashed on LLM failure: {e}")

if __name__ == '__main__':
    unittest.main()
